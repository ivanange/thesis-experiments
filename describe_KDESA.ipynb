{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from load_features import get_feature_extractor\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import match_descriptors, hog, SIFT\n",
    "from kernel_descriptors_extractor import KernelDescriptorsExtractor\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images dataset csv file\n",
    "# dataset = pd.read_csv('./dataset/test-dataset.csv')\n",
    "dataset = pd.read_csv('./dataset/dataset.csv')\n",
    "\n",
    "# set random state\n",
    "np.random.seed(0)\n",
    "\n",
    "# define image transforms\n",
    "# create transformations with different values of scale: 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5, rotation: 30, 60, 90, 150, 180, 270, translation: (10,0), (20, 40), (40, 80), (80, 160), (-10, 10), (-20, -40), (-40, -80), (-80, -160), shear: 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5\n",
    "transform_combinations = [\n",
    "    {},\n",
    "    {'scale': 0.25},\n",
    "    {'scale': 0.5},\n",
    "    {'scale': 2},\n",
    "    {'scale': 2.5},\n",
    "    {'rotation': 60},\n",
    "    {'rotation': 90},\n",
    "    {'rotation': 150},\n",
    "    {'rotation': 180},\n",
    "    {'scale': 0.25, 'rotation': 90},\n",
    "    {'scale': 0.25, 'rotation': 150},\n",
    "    {'scale': 0.25, 'rotation': 180},\n",
    "    {'scale': 2, 'rotation': 90},\n",
    "    {'scale': 2, 'rotation': 150},\n",
    "    {'scale': 2, 'rotation': 180},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize descriptors\n",
    "KDESAExtractor = KernelDescriptorsExtractor()\n",
    "SIFTExtractor = SIFT()\n",
    "descriptor_names = {\n",
    "    # 'SIFT': lambda image: ([SIFTExtractor.detect_and_extract(image), np.array(SIFTExtractor.descriptors).flatten()])[1],\n",
    "    # 'HOG': lambda image: hog(image, feature_vector=True, channel_axis=2),\n",
    "    'KDESA': lambda image: KDESAExtractor.predict(np.array([image]))[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   for each image transform combination\n",
    "#       for each descriptor\n",
    "#           create empty features array of size (n_images)\n",
    "#           for each image\n",
    "#               apply image transform combination\n",
    "#               extract features, remember max_feature_size\n",
    "#           for each row in features\n",
    "#               pad features to max_feature_size\n",
    "#           concatenate features to dataset\n",
    "#       save dataset to file as transform_combination_descriptor.csv\n",
    "\n",
    "# for each descriptor\n",
    "for (descriptor_name, descriptor) in descriptor_names.items():\n",
    "\n",
    "    max_full_feature_size = 0\n",
    "    full_features = list()\n",
    "\n",
    "    try:\n",
    "        # for each image transform combination\n",
    "        for transform_combination in transform_combinations:\n",
    "\n",
    "            # create empty features array of size (n_images)\n",
    "            features = list()\n",
    "            max_feature_size = 0\n",
    "\n",
    "            # for each image: apply transform combination, extract features\n",
    "            for index, row in dataset.iterrows():\n",
    "                try:\n",
    "                    # apply image transform combination\n",
    "                    image = io.imread(os.path.join(\n",
    "                        'dataset', row['file_path']))\n",
    "                    image = transform.warp(image, transform.AffineTransform(\n",
    "                        **transform_combination).inverse)\n",
    "\n",
    "                    # convert image to grayscale if not using kernel descriptors\n",
    "                    image = exposure.adjust_gamma(image, 2)\n",
    "                    if descriptor_name == 'SIFT':\n",
    "                        image = rgb2gray(image)\n",
    "\n",
    "                    # extract features, remember max_feature_size\n",
    "                    feature = descriptor(image)\n",
    "                    # print(feature.shape)\n",
    "                    # print(feature)\n",
    "                    features.append(feature)\n",
    "                    max_feature_size = max(max_feature_size, len(feature))\n",
    "                except Exception as e:\n",
    "                    features.append(np.zeros(max_feature_size))\n",
    "                    error = True\n",
    "                    print(f\"Error extracting features for {row['file_path']}\")\n",
    "                    print(e)\n",
    "\n",
    "            # pad each row in features to max_feature_size\n",
    "            for (i, feature) in enumerate(features):\n",
    "                features[i] = np.pad(feature, pad_width=(\n",
    "                    0, max_feature_size - len(feature)), mode='constant', constant_values=0)\n",
    "\n",
    "            # convert features to numpy array\n",
    "            features_df = np.asarray(features, dtype=np.float32)\n",
    "            # print(np.shape(features))\n",
    "\n",
    "            # create features dataset\n",
    "            features_df = pd.DataFrame(features)\n",
    "            features_df['category'] = dataset['category']\n",
    "            features_df['time'] = dataset['time']\n",
    "            features_df['invalid'] = dataset['censored'] == 1\n",
    "\n",
    "            filename = f\"{descriptor_name}_{'-'.join(map(lambda item : f'{item[0]}_{item[1]}', transform_combination.items()))}.csv\"\n",
    "            print(filename)\n",
    "            print(features_df.shape)\n",
    "\n",
    "            # save dataset to file as {transform_combination}_{descriptor_name}.csv\n",
    "            features_df.to_csv(os.path.join(\n",
    "                'features',  filename), index=False, header=False)\n",
    "            features_df = None\n",
    "\n",
    "            # concatenate features to full_features\n",
    "            full_features = full_features + features\n",
    "            # update max_full_feature_size\n",
    "            max_full_feature_size = max(\n",
    "                max_full_feature_size, max_feature_size)\n",
    "\n",
    "        # pad each row in full_features to max_full_feature_size\n",
    "        for (i, feature) in enumerate(full_features):\n",
    "            full_features[i] = np.pad(feature, pad_width=(\n",
    "                0, max_full_feature_size - len(feature)), mode='constant', constant_values=0)\n",
    "\n",
    "        # convert full_features to numpy array\n",
    "        full_features = np.asarray(full_features, dtype=np.float32)\n",
    "\n",
    "        # create full_features dataset\n",
    "        n_transform_combinations = len(transform_combinations)\n",
    "        full_features = pd.DataFrame(full_features)\n",
    "        full_features['category'] = list(\n",
    "            dataset['category']) * n_transform_combinations\n",
    "        full_features['time'] = list(\n",
    "            dataset['time']) * n_transform_combinations\n",
    "        full_features['invalid'] = list(\n",
    "            dataset['censored'] == 1) * n_transform_combinations\n",
    "\n",
    "        filename = f\"{descriptor_name}_augmented.csv\"\n",
    "        print(filename)\n",
    "        print(full_features.shape)\n",
    "        # save dataset to file as {descriptor_name}_augmented.csv\n",
    "        full_features.to_csv(os.path.join(\n",
    "            'features', filename), index=False, header=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error extracting features for {descriptor_name} {transform_combination}\")\n",
    "        print(e)\n",
    "        error = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0184c18853e8e47cfb82ea90866ea80d762664769a021ef45f73f671c1a896cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
